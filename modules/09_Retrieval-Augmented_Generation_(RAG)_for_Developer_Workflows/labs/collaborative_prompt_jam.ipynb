{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üë• Collaborative LLM Development Notebook\n",
    "This notebook reinforces Module 09 by:\n",
    "- Practicing prompt evaluation\n",
    "- Comparing peer prompt results\n",
    "- Logging contributions\n",
    "- Simulating a team checkpoint process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Step 1: Prompt Evaluation Log"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "team_prompts = [\n",
    "    \"How do I create an async generator in Python?\",\n",
    "    \"What does the go! macro do in Rust?\",\n",
    "    \"Generate a coroutine to stream logs to disk\"\n",
    "]\n",
    "\n",
    "for p in team_prompts:\n",
    "    print(\"‚úÖ Prompt:\", p)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 2: Generate Outputs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import pipeline\n",
    "model = pipeline(\"text-generation\", model=\"./checkpoints/my_model\")\n",
    "\n",
    "outputs = []\n",
    "for prompt in team_prompts:\n",
    "    result = model(prompt, max_new_tokens=100)[0]['generated_text']\n",
    "    outputs.append({\"prompt\": prompt, \"output\": result})\n",
    "    print(\"\\n\\nPrompt:\", prompt)\n",
    "    print(\"Output:\\n\", result)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Step 3: Evaluate with Team Tags (‚úÖ ‚ö†Ô∏è ‚ùå)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Replace with manual notes or teammate input\n",
    "eval_log = []\n",
    "for item in outputs:\n",
    "    rating = input(f\"Rate output for: {item['prompt']} (‚úÖ ‚ö†Ô∏è ‚ùå): \")\n",
    "    eval_log.append({\"prompt\": item['prompt'], \"rating\": rating})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Step 4: Contribute New Prompt"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "new_prompt = input(\"Add a new team prompt: \")\n",
    "contributor = input(\"Your name: \")\n",
    "\n",
    "Path(\"data/internal_curated\").mkdir(parents=True, exist_ok=True)\n",
    "with open(\"data/internal_curated/prompts.jsonl\", \"a\") as f:\n",
    "    f.write(json.dumps({\"instruction\": new_prompt, \"contributor\": contributor}) + \"\\n\")\n",
    "print(\"‚úÖ Prompt saved\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 5: Write a Checkpoint README"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "readme = \"\"\"# Team Model Checkpoint v1.1\n",
    "\n",
    "- Dataset: module_06 delta + prompt jam additions\n",
    "- Trained by: Alice, Bob, Charlie\n",
    "- Date: 2025-06-12\n",
    "- Metrics: Accuracy 83%, Hallucination Rate: 12%\n",
    "- Known Gaps: Streaming generation not supported yet.\n",
    "\"\"\"\n",
    "Path(\"models/team_checkpoints/model_v1.1\").mkdir(parents=True, exist_ok=True)\n",
    "with open(\"models/team_checkpoints/model_v1.1/README.md\", \"w\") as f:\n",
    "    f.write(readme)\n",
    "print(\"‚úÖ README created\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

