{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4006350e",
   "metadata": {},
   "source": [
    "# üîç RAG Demo Notebook: Retrieval-Augmented Generation\n",
    "This notebook walks through:\n",
    "- Embedding a document corpus\n",
    "- Retrieving top matches per query\n",
    "- Assembling prompts with retrieval\n",
    "- Generating grounded answers\n",
    "- Tracking citation sources and chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, faiss\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c922e91",
   "metadata": {},
   "source": [
    "## üìÅ Step 1: Load and Chunk Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0771db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = Path(\"data/corpus\")\n",
    "files = list(corpus_path.glob(\"*.md\"))\n",
    "chunks, sources = [], []\n",
    "\n",
    "for file in files:\n",
    "    text = file.read_text()\n",
    "    for para in text.split(\"\\n\\n\"):\n",
    "        if len(para.split()) > 20:\n",
    "            chunks.append(para.strip())\n",
    "            sources.append(file.name)\n",
    "\n",
    "print(f\"Loaded {len(chunks)} content chunks from {len(files)} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3800204b",
   "metadata": {},
   "source": [
    "## üî¢ Step 2: Embed and Index Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881394f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embedder.encode(chunks, show_progress_bar=True)\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4ce219",
   "metadata": {},
   "source": [
    "## üîç Step 3: Define RAG Query Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098335db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(query, k=3):\n",
    "    q_embed = embedder.encode([query])\n",
    "    _, indices = index.search(q_embed, k)\n",
    "    top_chunks = [chunks[i] for i in indices[0]]\n",
    "    top_sources = [sources[i] for i in indices[0]]\n",
    "    return top_chunks, top_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064ae1fe",
   "metadata": {},
   "source": [
    "## üß† Step 4: Assemble Prompt with Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d06f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(chunks, query):\n",
    "    context = \"\\n\\n\".join(chunks)\n",
    "    return f\"Answer the following based only on the provided context.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5d69cb",
   "metadata": {},
   "source": [
    "## ü§ñ Step 5: Generate Grounded Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625dda28",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_model = pipeline(\"text-generation\", model=\"./checkpoints/my_model\")\n",
    "\n",
    "query = \"How does the coroutine decorator work?\"\n",
    "chunks_used, cited_sources = rag_query(query)\n",
    "prompt = format_prompt(chunks_used, query)\n",
    "answer = rag_model(prompt, max_new_tokens=200)[0]['generated_text']\n",
    "\n",
    "display(Markdown(f\"**Query:** {query}\"))\n",
    "display(Markdown(f\"**Answer:**\\n\\n{answer}\"))\n",
    "display(Markdown(f\"**Citations:** {', '.join(set(cited_sources))}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cdbca1",
   "metadata": {},
   "source": [
    "## üí¨ (Optional) Chat History Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43798108",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "def ask_rag(query):\n",
    "    chunks_used, sources_used = rag_query(query)\n",
    "    full_prompt = format_prompt(chunks_used, query)\n",
    "    response = rag_model(full_prompt, max_new_tokens=150)[0]['generated_text']\n",
    "    chat_history.append({\n",
    "        'query': query,\n",
    "        'response': response.strip(),\n",
    "        'sources': sources_used\n",
    "    })\n",
    "    display(Markdown(f\"**You:** {query}\"))\n",
    "    display(Markdown(f\"**Bot:** {response.strip()}\\n\\n**From:** {', '.join(set(sources_used))}\"))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
