{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Hallucination Evaluation Notebook\n",
    "This notebook evaluates your model's outputs against known instructions and expected outputs.\n",
    "It tracks hallucinations, summarizes accuracy, and visualizes the results.\n",
    "Includes support for pass@k and CSV export."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Load Known Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open('data/hallucination_cases.jsonl') as f:\n",
    "    cases = json.load(f)['cases']\n",
    "print(f'Loaded {len(cases)} evaluation cases.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Load Your Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "generator = pipeline(\"text-generation\", model=\"./checkpoints/my_model\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Evaluate and Score (Pass@k Enabled)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = []\n",
    "k = 3  # number of generations to sample per prompt\n",
    "\n",
    "for case in tqdm(cases):\n",
    "    prompt = case['instruction']\n",
    "    expected = case['expected']\n",
    "    completions = generator(prompt, max_new_tokens=128, num_return_sequences=k, do_sample=True)\n",
    "    outputs = [c['generated_text'] for c in completions]\n",
    "    passed = any(expected in out for out in outputs)\n",
    "\n    results.append({\n",
    "        'prompt': prompt,\n",
    "        'expected': expected,\n",
    "        'outputs': outputs,\n",
    "        'hallucinated': not passed,\n",
    "        'reason': case['reason'] if not passed else ''\n    })"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualize and Save Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.DataFrame(results)\n",
    "display(df[['prompt', 'hallucinated', 'reason']])\n",
    "\n",
    "hallucination_counts = df['hallucinated'].value_counts()\n",
    "labels = ['Correct', 'Hallucinated']\n",
    "values = [hallucination_counts.get(False, 0), hallucination_counts.get(True, 0)]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(labels, values, color=[\"#4CAF50\", \"#F44336\"])\n",
    "plt.title(\"Hallucination Evaluation Results\")\n",
    "plt.ylabel(\"Number of Outputs\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Hallucination Rate:', df['hallucinated'].mean())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Export Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "Path('logs').mkdir(exist_ok=True)\n",
    "df.to_json('logs/hallucination_eval_results.json', indent=2)\n",
    "df.to_csv('logs/hallucination_eval_results.csv', index=False)\n",
    "print('‚úÖ Exported results to logs/hallucination_eval_results.*')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

