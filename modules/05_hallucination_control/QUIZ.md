# ðŸ§ª Module 05 Quiz: Minimizing Hallucinations in Code Generation

Test your ability to recognize and mitigate hallucinations in LLM-generated code.

---

### âœ… Instructions

* Choose the best answer for each multiple-choice question.
* Reflect on open-ended questions to connect learning with your goals.

---

### 1. What is a hallucination in the context of code generation?

A. A failed compilation
B. A confident, incorrect or fabricated output
C. Any code with comments
D. When the model takes longer than expected

**Correct Answer:** B

---

### 2. Which of the following is an example of hallucination?

A. Correctly printing to stdout
B. Making an API call to a real SDK method
C. Calling a function that does not exist
D. Returning a 200 OK HTTP response

**Correct Answer:** C

---

### 3. What can reduce hallucination during training?

A. Using higher learning rates
B. Adding more obscure prompts
C. Providing accurate, well-structured datasets
D. Using random sampling during inference

**Correct Answer:** C

---

### 4. Which strategy is useful to *detect* hallucination?

A. Static analysis and test prompts
B. Using a GPU with more VRAM
C. Swapping tokenizers at runtime
D. Removing all metadata

**Correct Answer:** A

---

### 5. Reflection:

> Think of a case where a code assistant gave you incorrect advice. How could you frame that failure as a prompt example to teach the model?

Write a sample instruction + corrected output in your journal or as a `.jsonl` entry.
