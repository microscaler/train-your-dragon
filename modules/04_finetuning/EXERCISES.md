# ðŸ§ª Module 04 â€“ Practice Exercises: Fine-Tuning with Unsloth

These self-paced exercises help reinforce key skills in configuring and executing fine-tuning workflows.

---

## âœ… Exercise 1: Train on 1 Example (Debug Run)

* Create a `.jsonl` file with a single, known-good example
* Run `train_model.ipynb` using `num_train_epochs = 1`
* Confirm the model memorizes the output exactly

ðŸŽ¯ **Goal:** Understand overfitting as a way to debug training pipeline

---

## âœ… Exercise 2: Try a Different Base Model

* Replace Zephyr with another LoRA-compatible model (e.g., TinyLlama or DeepSeek)
* Modify `model_name` in the notebook or script
* Note any VRAM or performance differences

ðŸŽ¯ **Goal:** Learn how model choice affects memory and speed

---

## âœ… Exercise 3: Modify Training Parameters

* Change `per_device_train_batch_size`, `num_train_epochs`, and `learning_rate`
* Observe effects on training time and final loss

ðŸŽ¯ **Goal:** Develop intuition for hyperparameter tuning

---

## âœ… Exercise 4: Add `wandb` Logging

* Install with `pip install wandb`
* Add `report_to="wandb"` in `TrainingArguments`
* Watch metrics in real-time at [wandb.ai](https://wandb.ai)

ðŸŽ¯ **Goal:** Learn to monitor training with external tools

---

## âœ… Exercise 5: Reload and Test Model from Checkpoint

* Use `from_pretrained()` to load your saved model
* Generate completions from test prompts in `evaluate_model.ipynb`

ðŸŽ¯ **Goal:** Verify you can recover, reuse, and share trained models
