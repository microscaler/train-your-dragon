{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af00d64",
   "metadata": {},
   "source": [
    "# üßÆ Tokenizer Vocab Coverage Lab ‚Äì Module 11\n",
    "\n",
    "This notebook complements your tokenizer experiments with:\n",
    "- UNK detection\n",
    "- Vocabulary coverage scoring\n",
    "- Visualization of token fragmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c563cc77",
   "metadata": {},
   "source": [
    "## üîç Step 1: Load 100 Prompt Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import json\n",
    "\n",
    "samples = []\n",
    "with open(\"data/internal_curated/clean.jsonl\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        samples.append(item['instruction'])\n",
    "\n",
    "random.seed(42)\n",
    "samples = random.sample(samples, min(100, len(samples)))\n",
    "print(samples[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e93e9a2",
   "metadata": {},
   "source": [
    "## üî¢ Step 2: Compare Token Count per Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0077dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_tok = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "# Replace with your own tokenizer path\n",
    "# from tokenizers import Tokenizer\n",
    "# custom_tok = Tokenizer.from_file(\"tokenizers/custom-vocab/tokenizer.json\")\n",
    "\n",
    "base_counts = [len(base_tok.encode(line)) for line in samples]\n",
    "# custom_counts = [len(custom_tok.encode(line).ids) for line in samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd92aeb",
   "metadata": {},
   "source": [
    "## üìä Step 3: Visualize Token Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc18285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(base_counts, bins=20, alpha=0.7, label=\"Base tokenizer\")\n",
    "# plt.hist(custom_counts, bins=20, alpha=0.7, label=\"Custom tokenizer\")\n",
    "plt.legend()\n",
    "plt.title(\"Token Distribution per Prompt\")\n",
    "plt.xlabel(\"Tokens\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28680a0a",
   "metadata": {},
   "source": [
    "## üßæ Step 4: Avg and Fragmentation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac96d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Base tokenizer:\")\n",
    "print(f\"Min: {min(base_counts)} | Max: {max(base_counts)} | Avg: {sum(base_counts)/len(base_counts):.2f}\")\n",
    "\n",
    "# Uncomment below if comparing\n",
    "# print(\"Custom tokenizer:\")\n",
    "# print(f\"Min: {min(custom_counts)} | Max: {max(custom_counts)} | Avg: {sum(custom_counts)/len(custom_counts):.2f}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
