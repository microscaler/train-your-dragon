{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Fine-Tuning with Unsloth: Guided Notebook\n",
    "This notebook walks you through fine-tuning a small LLM using Unsloth.\n",
    "We will:\n",
    "- Load a JSONL dataset\n",
    "- Initialize a PEFT-compatible model\n",
    "- Configure training\n",
    "- Save and reload the model\n",
    "- Generate outputs to verify results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 1: Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate datasets unsloth peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Step 2: Load Your Dataset\n",
    "Ensure your file follows Alpaca-style JSONL with `instruction` and `output`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "raw = load_dataset('json', data_files='data/custom_training.jsonl')['train']\n",
    "raw = raw.map(lambda ex: {'prompt': ex['instruction'], 'completion': ex['output']})\n",
    "print(raw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Step 3: Initialize the Model with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name='unsloth/zephyr-1.3b-bnb-4bit',\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 4: Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "args = TrainingArguments(\n",
    "    output_dir='./checkpoints/my_model',\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy='epoch'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 5: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "model.train_model(dataset=raw, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 6: Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save_pretrained('./checkpoints/my_model')\n",
    "# To reload later:\n",
    "# model = model.from_pretrained('./checkpoints/my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Step 7: Generate a Sample Output"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = \"Explain coroutines in Python\"\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(model.device)\n",
    "out = model.generate(input_ids, max_new_tokens=100)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

