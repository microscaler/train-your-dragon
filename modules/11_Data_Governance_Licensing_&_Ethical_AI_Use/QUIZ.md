# ðŸ§ª Module 11 Quiz: Tokenizer Engineering & Vocab Optimization

Test your understanding of tokenizer types, vocab tradeoffs, and model integration.

---

### âœ… Instructions

Choose the best answer per question. Reflect in your notebook when prompted.

---

### 1. Which of the following is NOT a common tokenizer type?

A. BPE (Byte Pair Encoding)
B. WordPiece
C. Unigram
D. ByteTreeSort

**Correct Answer:** D

---

### 2. What does high token fragmentation usually indicate?

A. Excellent vocabulary fit
B. Efficient training throughput
C. Poor vocabulary coverage
D. Overfitting on large sequences

**Correct Answer:** C

---

### 3. Which file type is generated by SentencePiece?

A. `.vocab`
B. `.model`
C. Both A and B
D. `.txt` only

**Correct Answer:** C

---

### 4. Why might you train a custom tokenizer?

A. To remove line breaks
B. To reduce prompt length for your domain
C. To disable attention masking
D. To avoid training altogether

**Correct Answer:** B

---

### 5. Reflection:

> Find one `.py` or `.jsonl` file and tokenize it with two different vocabularies. Which one gives fewer tokens and why?
